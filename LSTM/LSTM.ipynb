{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDownloading and preparing dataset Unknown size (download: Unknown size, generated: Unknown size, total: Unknown size) to C:\\Users\\sou05094\\tensorflow_datasets\\openbookqa\\0.1.0...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dl Completed...: 0 url [00:00, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:00<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:00<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:01<?, ? url/s]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:02<00:00,  2.00s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:02<00:00,  2.00s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:02<00:00,  2.00s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:02<00:00,  2.00s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:02<00:00,  2.00s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:02<00:00,  2.00s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:02<00:00,  2.00s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:02<00:00,  2.00s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:02<00:00,  2.00s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:02<00:00,  2.00s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:02<00:00,  2.00s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:02<00:00,  2.00s/ url]\n",
      "\u001b[A\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:02<00:00,  2.00s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:02<00:00,  2.00s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:02<00:00,  2.00s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:02<00:00,  2.00s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:02<00:00,  2.00s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:02<00:00,  2.00s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:02<00:00,  2.00s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:02<00:00,  2.00s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:02<00:00,  2.00s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:02<00:00,  2.00s/ url]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:02<00:00,  2.00s/ url]\n",
      "Extraction completed...: 100%|██████████| 11/11 [00:02<00:00,  5.26 file/s]\n",
      "Dl Size...: 100%|██████████| 1/1 [00:02<00:00,  2.09s/ MiB]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:02<00:00,  2.09s/ url]\n",
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDataset openbookqa downloaded and prepared to C:\\Users\\sou05094\\tensorflow_datasets\\openbookqa\\0.1.0. Subsequent calls will reuse this data.\u001b[0m\n",
      "Epoch 1/10\n",
      "155/155 [==============================] - 4s 21ms/step - loss: 1.3858 - accuracy: 0.2742\n",
      "Epoch 2/10\n",
      "155/155 [==============================] - 3s 21ms/step - loss: 1.3848 - accuracy: 0.2758\n",
      "Epoch 3/10\n",
      "155/155 [==============================] - 3s 22ms/step - loss: 1.3844 - accuracy: 0.2780\n",
      "Epoch 4/10\n",
      "155/155 [==============================] - 3s 22ms/step - loss: 1.3847 - accuracy: 0.2780\n",
      "Epoch 5/10\n",
      "155/155 [==============================] - 3s 22ms/step - loss: 1.3844 - accuracy: 0.2780\n",
      "Epoch 6/10\n",
      "155/155 [==============================] - 3s 22ms/step - loss: 1.3847 - accuracy: 0.2780\n",
      "Epoch 7/10\n",
      "155/155 [==============================] - 3s 22ms/step - loss: 1.3844 - accuracy: 0.2780\n",
      "Epoch 8/10\n",
      "155/155 [==============================] - 3s 22ms/step - loss: 1.3843 - accuracy: 0.2780\n",
      "Epoch 9/10\n",
      "155/155 [==============================] - 3s 22ms/step - loss: 1.3845 - accuracy: 0.2780\n",
      "Epoch 10/10\n",
      "155/155 [==============================] - 3s 22ms/step - loss: 1.3844 - accuracy: 0.2780\n",
      "1/1 [==============================] - 0s 217ms/step\n",
      "Question: What is the largest animal on Earth? / Predicted Label: 0\n",
      "Question: Who is the author of 'Harry Potter'? / Predicted Label: 0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "\n",
    "# OpenBookQA 데이터셋 빌더를 불러옵니다.\n",
    "builder = tfds.builder('openbookqa')\n",
    "\n",
    "# 데이터셋을 다운로드하고 준비합니다.\n",
    "builder.download_and_prepare()\n",
    "dataset = builder.as_dataset(split='train')\n",
    "\n",
    "# 데이터셋에서 텍스트와 라벨을 추출합니다.\n",
    "texts = []\n",
    "labels = []\n",
    "for example in dataset:\n",
    "    texts.append(example['question']['stem'].numpy().decode('utf-8'))\n",
    "    labels.append(example['answerKey'].numpy())\n",
    "\n",
    "# 텍스트를 정수 시퀀스로 변환하는 Tokenizer를 사용하여 전처리합니다.\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(texts)\n",
    "word_index = tokenizer.word_index\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "max_sequence_length = max(len(seq) for seq in sequences)\n",
    "\n",
    "# 패딩을 적용하여 시퀀스 길이를 동일하게 맞춥니다.\n",
    "sequences_padded = pad_sequences(sequences, maxlen=max_sequence_length, padding='post')\n",
    "\n",
    "# 라벨을 원-핫 인코딩합니다.\n",
    "labels_onehot = tf.keras.utils.to_categorical(labels)\n",
    "\n",
    "# LSTM 모델 정의\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(len(word_index) + 1, 64, input_length=max_sequence_length),\n",
    "    tf.keras.layers.LSTM(128),\n",
    "    tf.keras.layers.Dense(len(labels_onehot[0]), activation='softmax')\n",
    "])\n",
    "\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# LSTM 모델 학습\n",
    "model.fit(sequences_padded, labels_onehot, epochs=10, batch_size=32)\n",
    "\n",
    "# 새로운 텍스트를 예측하는 예시\n",
    "new_texts = [\"What is the largest animal on Earth?\", \"Who is the author of 'Harry Potter'?\"]\n",
    "new_sequences = tokenizer.texts_to_sequences(new_texts)\n",
    "new_sequences_padded = pad_sequences(new_sequences, maxlen=max_sequence_length, padding='post')\n",
    "predictions = model.predict(new_sequences_padded)\n",
    "\n",
    "for i, text in enumerate(new_texts):\n",
    "    predicted_label = np.argmax(predictions[i])\n",
    "    print(f\"Question: {text} / Predicted Label: {predicted_label}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "124/124 [==============================] - 9s 56ms/step - loss: 1.3870 - accuracy: 0.2704 - val_loss: 1.3824 - val_accuracy: 0.2893\n",
      "Epoch 2/10\n",
      "124/124 [==============================] - 7s 53ms/step - loss: 1.3856 - accuracy: 0.2704 - val_loss: 1.3838 - val_accuracy: 0.2893\n",
      "Epoch 3/10\n",
      "124/124 [==============================] - 6s 52ms/step - loss: 1.3854 - accuracy: 0.2757 - val_loss: 1.3823 - val_accuracy: 0.2893\n",
      "Epoch 4/10\n",
      "124/124 [==============================] - 7s 53ms/step - loss: 1.3855 - accuracy: 0.2729 - val_loss: 1.3819 - val_accuracy: 0.2893\n",
      "Epoch 5/10\n",
      "124/124 [==============================] - 7s 53ms/step - loss: 1.3856 - accuracy: 0.2752 - val_loss: 1.3834 - val_accuracy: 0.2893\n",
      "Epoch 6/10\n",
      "124/124 [==============================] - 7s 55ms/step - loss: 1.3854 - accuracy: 0.2752 - val_loss: 1.3833 - val_accuracy: 0.2893\n",
      "Epoch 7/10\n",
      "124/124 [==============================] - 7s 54ms/step - loss: 1.3855 - accuracy: 0.2739 - val_loss: 1.3818 - val_accuracy: 0.2893\n",
      "Epoch 8/10\n",
      "124/124 [==============================] - 7s 56ms/step - loss: 1.3849 - accuracy: 0.2699 - val_loss: 1.3826 - val_accuracy: 0.2893\n",
      "Epoch 9/10\n",
      "124/124 [==============================] - 7s 54ms/step - loss: 1.3852 - accuracy: 0.2752 - val_loss: 1.3829 - val_accuracy: 0.2893\n",
      "Epoch 10/10\n",
      "124/124 [==============================] - 7s 54ms/step - loss: 1.3851 - accuracy: 0.2752 - val_loss: 1.3820 - val_accuracy: 0.2893\n",
      "1/1 [==============================] - 0s 364ms/step\n",
      "Question: What is the largest animal on Earth? / Predicted Label: 0\n",
      "Question: Who is the author of 'Harry Potter'? / Predicted Label: 0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import LSTM, Bidirectional, Dropout, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# LSTM 모델 정의\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(len(word_index) + 1, 64, input_length=max_sequence_length),\n",
    "    LSTM(128, return_sequences=True),\n",
    "    Dropout(0.2),\n",
    "    LSTM(128),\n",
    "    Dropout(0.2),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(len(labels_onehot[0]), activation='softmax')\n",
    "])\n",
    "\n",
    "# 옵티마이저 설정\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# 모델 학습\n",
    "model.fit(sequences_padded, labels_onehot, epochs=10, batch_size=32, validation_split=0.2, verbose=1)\n",
    "\n",
    "# 새로운 텍스트를 예측하는 예시\n",
    "new_texts = [\"What is the largest animal on Earth?\", \"Who is the author of 'Harry Potter'?\"]\n",
    "new_sequences = tokenizer.texts_to_sequences(new_texts)\n",
    "new_sequences_padded = pad_sequences(new_sequences, maxlen=max_sequence_length, padding='post')\n",
    "predictions = model.predict(new_sequences_padded)\n",
    "\n",
    "for i, text in enumerate(new_texts):\n",
    "    predicted_label = np.argmax(predictions[i])\n",
    "    print(f\"Question: {text} / Predicted Label: {predicted_label}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
